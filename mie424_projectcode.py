# -*- coding: utf-8 -*-
"""MIE424_ProjectCode.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1xyOMHwDcv9kUAV6OESfBruO7ofxP8AnU
"""

import torchvision
import torchvision.transforms as transforms
import torch
from torch.utils import data
import torch.nn as nn
import math
import matplotlib.pyplot as plt
import torch.optim as optim
import numpy as np
import time
import gc
import torch.nn.functional as F
import helper
from sklearn import metrics
from sklearn.metrics import accuracy_score
from torchsummary import summary
from torch.utils.data import ConcatDataset
import os
from sklearn import metrics
import seaborn as sn
import pandas as pd
from sklearn.model_selection import train_test_split
from collections import Counter
from torch import autograd
from torch import nn
from torch.autograd import Variable

"""## Data Loader"""

# Because the dataset is extremely unbalanced, so I am doing a stratified split here 
# https://discuss.pytorch.org/t/how-to-do-a-stratified-split/62290
# https://stackoverflow.com/questions/40829137/stratified-train-validation-test-split-in-scikit-learn

torch.manual_seed(66)
np.random.seed(66)

# Rescale Everything to 50X50 px
transform_function = transforms.Compose(
        [
         transforms.Resize((50,50)),
         transforms.ToTensor(),
         ])

# Ref:https://discuss.pytorch.org/t/attributeerror-subset-object-has-no-attribute-targets/66564/4
class Custom_Subset(data.Dataset):
    """
    Subset of a dataset at specified indices.

    Arguments:
        dataset (Dataset): The whole Dataset
        indices (sequence): Indices in the whole set selected for subset
        labels(sequence) : targets as required for the indices. will be the same length as indices
    """
    def __init__(self, dataset, indices, labels):
        self.dataset = data.Subset(dataset, indices)
        self.targets = labels
    def __getitem__(self, idx):
        image = self.dataset[idx][0]
        target = self.targets[idx]
        return (image, target)
    def __len__(self):
        return len(self.targets)


def get_data_loader(batch_size):
  full_training_data = torchvision.datasets.ImageFolder(
    root="/content/drive/My Drive/MIE424 Final Project/cropped_img",
    transform=transform_function)
  
  # run train_test_split twice to stratified split dataset into train,validation and testing
  targets = full_training_data.targets
  # print(dict(Counter(targets)))
  train_idx, valid_test_idx= train_test_split(np.arange(len(targets)),test_size=0.2,shuffle=True,stratify=targets)
  train_targets = [targets[i] for i in train_idx]
  # print(dict(Counter(train_targets)))
  train_data = Custom_Subset(full_training_data,train_idx,train_targets)
  valid_test_targets = [targets[i] for i in valid_test_idx]
  # print(dict(Counter(valid_test_targets)))
  valid_test_data = Custom_Subset(full_training_data,valid_test_idx,valid_test_targets)

  valid_idx, test_idx= train_test_split(np.arange(len(valid_test_targets)),test_size=0.5,shuffle=True,stratify=valid_test_targets)
  valid_targets = [valid_test_targets[i] for i in valid_idx]
  
  test_targets = [valid_test_targets[i] for i in test_idx]
  valid_data = Custom_Subset(valid_test_data,valid_idx,valid_targets)
  test_data = Custom_Subset(valid_test_data,test_idx,test_targets)

  print("There are " + str(len(train_data)) + " training images, and the corresponding data distribution is: ")
  print(dict(Counter(train_data.targets)))
  print("There are " + str(len(valid_data)) + " validation images, and the corresponding data distribution is: ")
  print(dict(Counter(valid_data.targets)))
  print("There are " + str(len(test_data)) + " test images, and the corresponding data distribution is: ")
  print(dict(Counter(test_data.targets)))
  
  train_loader = torch.utils.data.DataLoader(train_data, batch_size=batch_size,num_workers=2)
  
  val_loader = torch.utils.data.DataLoader(valid_data, batch_size=batch_size,num_workers=2)

  test_loader = torch.utils.data.DataLoader(test_data, batch_size=batch_size,num_workers=2)
  
  return train_loader, val_loader, test_loader

train_loader, val_loader, test_loader = get_data_loader(batch_size=400)

# Training_dataset
plt.figure()
_, sp = plt.subplots(1,6,figsize=(20,20)) 
images, labels = next(iter(train_loader))
# Add this so I can get 2 samples from each class
image_counter = {0:0,1:0,2:0}
index_counter = 0
for i in range(400):
  temp_index = i
  temp_img = images[temp_index].numpy().transpose((1, 2, 0))
  if image_counter[labels[temp_index].item()] < 2:
    print(labels[temp_index])
    sp[index_counter].imshow(temp_img)
    image_counter[labels[temp_index].item()] += 1
    index_counter+=1
  
  if sum(image_counter.values()) == 6:
    break

# Validation_dataset
plt.figure()
_, sp = plt.subplots(1,6,figsize=(20,20)) 
images, labels = next(iter(val_loader))
# Add this so I can get 2 samples from each class
image_counter = {0:0,1:0,2:0}
index_counter = 0
for i in range(400):
  temp_index = i
  temp_img = images[temp_index].numpy().transpose((1, 2, 0))
  if image_counter[labels[temp_index].item()] < 2:
    print(labels[temp_index])
    sp[index_counter].imshow(temp_img)
    image_counter[labels[temp_index].item()] += 1
    index_counter+=1
  
  if sum(image_counter.values()) == 6:
    break

# Testing_dataset
plt.figure()
_, sp = plt.subplots(1,6,figsize=(20,20)) 
images, labels = next(iter(test_loader))
# Add this so I can get 2 samples from each class
image_counter = {0:0,1:0,2:0}
index_counter = 0
for i in range(400):
  temp_index = i
  temp_img = images[temp_index].numpy().transpose((1, 2, 0))
  if image_counter[labels[temp_index].item()] < 2:
    print(labels[temp_index])
    sp[index_counter].imshow(temp_img)
    image_counter[labels[temp_index].item()] += 1
    index_counter+=1
  
  if sum(image_counter.values()) == 6:
    break

"""## Model Building"""

# Base Model Code Ref: https://github.com/bollakarthikeya/LeNet-5-PyTorch/blob/master/lenet5_gpu.py
# Note Here: In the original LeNet, they used Tanh. But we used a modified LeNet 5 with Relu as activation function to accelerate the convergence during training. 
# Ref for this: https://sh-tsang.medium.com/paper-brief-review-of-lenet-1-lenet-4-lenet-5-boosted-lenet-4-image-classification-1f5f809dbf17

class LeNet5(torch.nn.Module):          
  def __init__(self,num_class=3):     
    super(LeNet5, self).__init__()
    # [(Wâˆ’K+2P)/S]+1
    self.conv1 = torch.nn.Conv2d(in_channels=3, out_channels=6, kernel_size=5, stride=1, padding=2, bias=True)
    self.avg_pool = torch.nn.AvgPool2d(kernel_size = 2, stride = 2, padding = 0)
    self.conv2 = torch.nn.Conv2d(in_channels=6, out_channels=16, kernel_size=5, stride=1, padding=0, bias=True)
    self.fc1 = torch.nn.Linear(16*10*10, 240)   
    self.fc2 = torch.nn.Linear(240, 120)       
    self.fc3 = torch.nn.Linear(120, num_class)
      
  def forward(self, x):
    # first conv with ReLU non-linearity
    # print(x.shape)
    x = torch.nn.functional.relu(self.conv1(x))  
    # 2x2 avg_pool
    # print(x.shape)
    x = self.avg_pool(x)
    # print(x.shape) 
    # second conv with ReLU non-linearity
    x = torch.nn.functional.relu(self.conv2(x))
    # print(x.shape)
    # 2x2 avg_pool
    x = self.avg_pool(x)
    # print(x.shape)
    # flatten out the output
    x = x.view(-1, 16*10*10)
    # first fc with ReLU non-linearity
    x = torch.nn.functional.relu(self.fc1(x))
    # second fc with ReLU non-linearity
    x = torch.nn.functional.relu(self.fc2(x))
    # output result 
    x = self.fc3(x)
    
    return x

"""##Training Code"""

# REF https://www.cs.toronto.edu/~lczhang/aps360_20191/hw/a2/a2code.py

def get_model_name(name, batch_size, learning_rate, epoch):
    path = "model_{0}_bs{1}_lr{2}_epoch{3}".format(name,
                                                   batch_size,
                                                   learning_rate,
                                                   epoch)
    return path

def plot_training_curve(path):
    # Plots the training curve for a model run, given the csv files containing the train/validation error/loss.
    train_err = np.loadtxt("{}_train_err.csv".format(path))
    val_err = np.loadtxt("{}_val_err.csv".format(path))
    train_loss = np.loadtxt("{}_train_loss.csv".format(path))
    val_loss = np.loadtxt("{}_val_loss.csv".format(path))
    plt.title("Train vs Validation Error")
    plt.style.use('default')
    n = len(train_err) # number of epochs
    plt.plot(range(1,n+1), train_err, label="Train")
    plt.plot(range(1,n+1), val_err, label="Validation")
    plt.xlabel("Epoch")
    plt.ylabel("Error")
    plt.legend(loc='best')
    plt.show()
    plt.title("Train vs Validation Loss")
    plt.plot(range(1,n+1), train_loss, label="Train")
    plt.plot(range(1,n+1), val_loss, label="Validation")
    plt.xlabel("Epoch")
    plt.ylabel("Loss")
    plt.legend(loc='best')
    plt.show()

def evaluate(net, loader, criterion):
  total_loss = 0.0
  total_err = 0.0
  total_epoch = 0
  for i, data in enumerate(loader, 0):
      inputs, labels = data
      inputs = inputs.cuda()
      labels = labels.cuda()
      outputs = net(inputs)
      loss = criterion(outputs, labels.long())
      _, predicted = torch.max(outputs.data, 1)
      error_count = (predicted != labels).sum().item()
      total_err += int(error_count)
      total_loss += loss.item()
      total_epoch += labels.size(0)

  err = float(total_err) / total_epoch
  loss = float(total_loss) / (i + 1)
  return err, loss

def train_model(net, model_name, loss_function, batch_size=32, learning_rate=0.0001, num_epochs=30, print_stack_trace=True,save_output=False, output_path="/content/drive/My Drive/MIE424 Final Project/model_output/", using_cifar = False):
  torch.manual_seed(1000)
  if using_cifar:
    train_loader, val_loader, test_loader = get_data_loader_cifar_10_LT(batch_size=batch_size)
  else:
    train_loader, val_loader, test_loader = get_data_loader(batch_size=batch_size)
  # Keep Track of Best Performed Model:
  performance_dict = dict()
  performance_dict["best_val_err"] = ["model_name", 10000, 10000]
  performance_dict["best_val_loss"] = ["model_name", 10000, 10000]

  criterion = loss_function.cuda()
  optimizer = optim.Adam(net.parameters(), lr=learning_rate)
  train_err = np.zeros(num_epochs)
  train_loss = np.zeros(num_epochs)
  val_err = np.zeros(num_epochs)
  val_loss = np.zeros(num_epochs)

  # Train the network
  start_time = time.time()
  for epoch in range(num_epochs):
      total_train_loss = 0.0
      total_train_err = 0.0
      total_epoch = 0
      for i, data in enumerate(train_loader, 0):
        gc.collect()
        torch.cuda.empty_cache()
        inputs, labels = data
        inputs = inputs.cuda()
        labels = labels.cuda()
        optimizer.zero_grad()
        outputs = net(inputs)
        loss = criterion(outputs, labels.long())
        loss.backward()
        optimizer.step()
        optimizer.zero_grad()

        _, predicted = torch.max(outputs.data, 1)
        error_count = (predicted != labels).sum().item()
        total_train_err += int(error_count)


        total_train_loss += loss.item()
        total_epoch += labels.size(0)
        # print('Finished 1 batch')

        torch.cuda.empty_cache()

      train_err[epoch] = float(total_train_err) / total_epoch
      train_loss[epoch] = float(total_train_loss) / (i+1)
      val_err[epoch], val_loss[epoch] = evaluate(net, val_loader, criterion)
      if print_stack_trace:
        print(("Epoch {}: Train err: {}, Train loss: {} |"+
                "Validation err: {}, Validation loss: {}").format(
                    epoch + 1,
                    train_err[epoch],
                    train_loss[epoch],
                    val_err[epoch],
                    val_loss[epoch]))
      model_path = get_model_name(model_name, batch_size, learning_rate, epoch)
      
      if val_err[epoch] < performance_dict["best_val_err"][1]:
        performance_dict["best_val_err"] = [model_path, val_err[epoch], val_loss[epoch]]
        model_file_name = output_path + model_name + "best_val_err"
        if save_output:
          torch.save(net.state_dict(), model_file_name)
      if val_loss[epoch] < performance_dict["best_val_loss"][2]:
        performance_dict["best_val_loss"] = [model_path, val_err[epoch], val_loss[epoch]]
        model_file_name = output_path +  model_name + "best_val_loss"
        if save_output:
          torch.save(net.state_dict(), model_file_name)

  print('Finished Training: {}'.format(model_name))
  end_time = time.time()
  elapsed_time = end_time - start_time
  print("Total time elapsed: {:.2f} seconds".format(elapsed_time))
  print("The best model based on validation err is {}, its val_error is {} and val_loss is {}".format(performance_dict["best_val_err"][0],performance_dict["best_val_err"][1],performance_dict["best_val_err"][2]))
  print("The best model based on validation loss is {}, its val_error is {} and val_loss is {}".format(performance_dict["best_val_loss"][0],performance_dict["best_val_loss"][1],performance_dict["best_val_loss"][2]))
  torch.cuda.empty_cache()

  plt.title("Train vs Validation Error")
  n = len(train_err) # number of epochs
  plt.plot(range(1,n+1), train_err, label="Train")
  plt.plot(range(1,n+1), val_err, label="Validation")
  plt.xlabel("Epoch")
  plt.ylabel("Error")
  plt.legend(loc='best')
  plt.style.use('default')
  plt.show()
  plt.title("Train vs Validation Loss")
  plt.plot(range(1,n+1), train_loss, label="Train")
  plt.plot(range(1,n+1), val_loss, label="Validation")
  plt.xlabel("Epoch")
  plt.ylabel("Loss")
  plt.legend(loc='best')
  plt.style.use('default')
  plt.show()

  train_err_file_name = output_path + "{}_train_err.csv"
  train_loss_file_name = output_path + "{}_train_loss.csv"  
  val_err_file_name = output_path + "{}_val_err.csv"
  val_loss_file_name = output_path + "{}_val_loss.csv"

  if save_output:
    np.savetxt(train_err_file_name.format(model_path), train_err)
    np.savetxt(train_loss_file_name.format(model_path), train_loss)
    np.savetxt(val_err_file_name.format(model_path), val_err)
    np.savetxt(val_loss_file_name.format(model_path), val_loss)

"""##Hyperparameter Tuning
To have a reasonable hyperparameter setup for loss function evaluation

"""

# Check the impact of different learning rate on the Default_LeNet5 model:
learning_rate_to_try = [0.000001,0.00001,0.0001,0.001,0.005,0.01]
for i in learning_rate_to_try:
  model_name = "Default_LeNet5_LR_Tuning_" + str(i)
  gc.collect()
  torch.cuda.empty_cache()
  lenet = LeNet5()
  lenet.cuda()
  loss_function = nn.CrossEntropyLoss()
  train_model(lenet,model_name,batch_size=32, num_epochs=50, learning_rate=i,loss_function=loss_function,save_output=True,print_stack_trace=False)

# Check the impact of different batch number on the Default_LeNet5 model with lr=0.0001:
batch_size_to_try = [8,16,32,64,128,256]
for i in batch_size_to_try:
  model_name = "Default_LeNet5_BS_Tuning_" + str(i)
  gc.collect()
  torch.cuda.empty_cache()
  lenet = LeNet5()
  lenet.cuda()
  loss_function = nn.CrossEntropyLoss()
  train_model(lenet,model_name,batch_size=i, num_epochs=50, learning_rate=0.0001,loss_function=loss_function,save_output=True,print_stack_trace=False)

# To investigate if we need more epochs to stablize the result, and the output will not be saved here
model_name = "Default_LeNet5_With_Larger_Epochs_"
gc.collect()
torch.cuda.empty_cache()
lenet = LeNet5()
lenet.cuda()
loss_function = nn.CrossEntropyLoss()
train_model(lenet,model_name,batch_size=64, num_epochs=500, learning_rate=0.0001,loss_function=loss_function,save_output=False,print_stack_trace=False)

"""In this case, batch_size=64, learning_rate=0.0001, will become our final setup for loss function evaluation

## Loss Function

#### Cross Entropy Loss
"""

class CrossEntropyLoss_IMP(nn.Module):

  def __init__(self):
    super().__init__()

  def forward(self, inputs, targets):
    # Convert output to prob through softmax
    softmax_prob = inputs.exp() / (inputs.exp().sum(-1)).unsqueeze(-1)
    # Math calculation
    loss = -softmax_prob[range(targets.shape[0]), targets].log().mean()
    return loss

"""#### Weighted Cross Entropy Loss"""

class Weighted_CrossEntropyLoss_IMP(nn.Module):

  def __init__(self, weights):
      super().__init__()
      self.weights = weights

  def forward(self, inputs, targets):
      softmax_prob = inputs.exp() / (inputs.exp().sum(-1)).unsqueeze(-1)
      weight = torch.FloatTensor([self.weights]*len(inputs)).cuda()
      loss = -torch.mul(softmax_prob[range(targets.shape[0]), targets].log(), weight[range(targets.shape[0]), targets]).mean()
      return loss

"""#### Focal Loss"""

# Paper Ref:
class FocalLoss_IMP(nn.Module):
    def __init__(self, gamma=2):
        super(FocalLoss_IMP, self).__init__()
        self.gamma = gamma
        
    def forward(self, inputs, targets):
        # print(targets)
        d_0 = inputs.size(0) 
        d_1 = inputs.size(1)
        P = inputs.exp() / (inputs.exp().sum(-1)).unsqueeze(-1)

        # convert the label to 0 1
        #Ref https://github.com/laughtervv/Deeplab-Pytorch/tree/master/models
        class_mask = inputs.data.new(d_0, d_1).fill_(0)
        class_mask = Variable(class_mask)
        ids = targets.view(-1, 1)
        class_mask.scatter_(1, ids.data, 1.)  
        probs = (P*class_mask).sum(1).view(-1,1)

        log_p = P[range(targets.shape[0]), targets].log()
        batch_loss = -(torch.pow((1-probs), self.gamma))*log_p 
        loss = batch_loss.mean()
        return loss

"""#### SparseMax Loss"""

# Paper Ref: https://arxiv.org/pdf/1602.02068.pdf
# This loss function utilize the sparse transformation created by KrisKorrel
# Ref https://github.com/KrisKorrel/sparsemax-pytorch/blob/master/sparsemax.py

device = torch.device("cuda")
class Sparsemax_Loss_IMP(torch.nn.Module):

    def __init__(self):
        super(Sparsemax_Loss_IMP, self).__init__()

    def forward(self, input, target):
        input = input.transpose(0, -1)
        original_size = input.size()
        input = input.reshape(input.size(0), -1)
        input = input.transpose(0, 1)
        dim = 1
        number_of_logits = input.size(dim)
        input = input - torch.max(input, dim=dim, keepdim=True)[0].expand_as(input)
        zs = torch.sort(input=input, dim=dim, descending=True)[0]
        range = torch.arange(start=1, end=number_of_logits + 1, step=1, device=device, dtype=input.dtype).view(1, -1)
        range = range.expand_as(zs)
        bound = 1 + range * zs
        cumulative_sum_zs = torch.cumsum(zs, dim)
        is_gt = torch.gt(bound, cumulative_sum_zs).type(input.type())
        k = torch.max(is_gt * range, dim, keepdim=True)[0]
        zs_sparse = is_gt * zs
        taus = (torch.sum(zs_sparse, dim, keepdim=True) - 1) / k
        taus = taus.expand_as(input)
        self.output = torch.max(torch.zeros_like(input), input - taus)
        output = self.output
        output = output.transpose(0, 1)
        output = output.reshape(original_size)
        output = output.transpose(0, -1)

        results = torch.sum(input * output, dim=1) - (0.5 * torch.sum((output ** 2), dim=1) - 0.5)
        all_rows = torch.arange(target.shape[0])
        results -= input[all_rows, target]
        return torch.mean(results)

"""#### LDAM"""

# REF Paper:

class LDAMLoss_IMP(nn.Module):

    def __init__(self, n, C, normalize = True):
        super(LDAMLoss_IMP, self).__init__()
        gamma = 1.0 / np.power(n, (1/4))
        # Ref https://github.com/kaidic/LDAM-DRW/blob/master/losses.py
        if normalize:
          normalized_C = C / np.max(gamma)
          gamma = gamma * normalized_C
        else:
          gamma = gamma * C
        gamma = torch.from_numpy(gamma)
        gamma = gamma.type(torch.FloatTensor)
        gamma.cuda()
        self.gamma = gamma

    def forward(self, inputs, targets):
        one_hot = torch.zeros((inputs.shape[0], inputs.shape[1]), dtype=torch.uint8).cuda()
        # convert the label to 0 1
        # Ref https://github.com/laughtervv/Deeplab-Pytorch/tree/master/models
        ids = targets.view(-1, 1)
        one_hot.scatter_(1, ids.data, 1.)
        target_gamma = (self.gamma@(one_hot.type(torch.FloatTensor)).transpose(0, 1)).view(-1, 1).cuda()
        regularized_x = inputs - target_gamma
        output = torch.where(one_hot, regularized_x, inputs)
        softmax_prob = output.exp() / (output.exp().sum(-1)).unsqueeze(-1)
        loss = -softmax_prob[range(targets.shape[0]), targets].log().mean()
        return loss

"""## Model Training"""

# CE
model_name = "Loss_Function_Evaluation_CrossEntropyLoss_IMP"
gc.collect()
torch.cuda.empty_cache()
lenet = LeNet5()
lenet.cuda()
loss_function = CrossEntropyLoss_IMP()
train_model(lenet,model_name,batch_size=64, num_epochs=100, learning_rate=0.0001,loss_function=loss_function,save_output=True,print_stack_trace=False)

# CE - PyTorch Implementation
model_name = "Loss_Function_Evaluation_CrossEntropyLoss_PYTORCH"
gc.collect()
torch.cuda.empty_cache()
lenet = LeNet5()
lenet.cuda()
loss_function = nn.CrossEntropyLoss()
train_model(lenet,model_name,batch_size=64, num_epochs=100, learning_rate=0.0001,loss_function=loss_function,save_output=True,print_stack_trace=False)

# Weighted CE PyTorch Implementation (Weight = Inverse of # class)
model_name = "Loss_Function_Evaluation_Weighted_CrossEntropyLoss_PYTORCH_Inver_Num"
gc.collect()
torch.cuda.empty_cache()
lenet = LeNet5()
lenet.cuda()
loss_function = nn.CrossEntropyLoss(weight=torch.FloatTensor([3257.0/98.0,3257.0/2585.0,3257.0/574.0]))
train_model(lenet,model_name,batch_size=64, num_epochs=100, learning_rate=0.0001,loss_function=loss_function,save_output=True,print_stack_trace=False)

# Weighted CE (Weight = Frequency)
model_name = "Loss_Function_Evaluation_Weighted_CrossEntropyLoss_IMP_Frequency"
gc.collect()
torch.cuda.empty_cache()
lenet = LeNet5()
lenet.cuda()
loss_function = Weighted_CrossEntropyLoss_IMP(weights=[98.0/3257.0,2585.0/3257.0,574.0/3257.0])
train_model(lenet,model_name,batch_size=64, num_epochs=100, learning_rate=0.0001,loss_function=loss_function,save_output=True,print_stack_trace=False)
# Weighted CE (Weight = Inverse class frequency)
model_name = "Loss_Function_Evaluation_Weighted_CrossEntropyLoss_IMP_Inverse_Frequency"
gc.collect()
torch.cuda.empty_cache()
lenet = LeNet5()
lenet.cuda()
loss_function = Weighted_CrossEntropyLoss_IMP(weights=[3257.0/98.0,3257.0/2585.0,3257.0/574.0])
train_model(lenet,model_name,batch_size=64, num_epochs=100, learning_rate=0.0001,loss_function=loss_function,save_output=True,print_stack_trace=False)

# Weighted CE (Weight = Linearly scaled k>0)
count = [98.0,2585.0,574.0]
weights_frequency = []
for i in count:
  weights_frequency.append(i/sum(count))

K = [0.1, 0.5, 1.2, 5, 10]
weights_linearly_scaled = []
for k in K:
  temp = []
  for i in weights_frequency:
    temp.append(k/i)
  k_name = str(k).replace(".","_")
  name = "Loss_Function_Evaluation_Weighted_CrossEntropyLoss_IMP_Linearly_Scaled_K_" + k_name
  model_name = name
  gc.collect()
  torch.cuda.empty_cache()
  lenet = LeNet5()
  lenet.cuda()
  loss_function = Weighted_CrossEntropyLoss_IMP(weights=temp)
  print("Parameters used in this run: ")
  print(temp)
  train_model(lenet,model_name,batch_size=64, num_epochs=100, learning_rate=0.0001,loss_function=loss_function,save_output=True,print_stack_trace=False)

# Weighted CE (Weight = logarithmic q>frequency)
count = [98.0,2585.0,574.0]
weights_frequency = []
for i in count:
  weights_frequency.append(i/sum(count))

Q = [0.5, 1, 2, 5, 10]
for q in Q:
  temp = []
  for i in weights_frequency:
    temp.append(math.log(q/i))
  q_name = str(q).replace(".","_")
  name = "Loss_Function_Evaluation_Weighted_CrossEntropyLoss_IMP_Logarithmic_Q_" + q_name
  model_name = name
  gc.collect()
  torch.cuda.empty_cache()
  lenet = LeNet5()
  lenet.cuda()
  loss_function = Weighted_CrossEntropyLoss_IMP(weights=temp)
  print("Parameters used in this run: ")
  print(temp)
  train_model(lenet,model_name,batch_size=64, num_epochs=100, learning_rate=0.0001,loss_function=loss_function,save_output=True,print_stack_trace=False)

# Weighted CE (Weight = Effective number of samples)
count = [98.0,2585.0,574.0]
Betas = [0.9,0.9,0.999,0.9999]
for b in Betas:
  temp = []
  for i in count:
    temp.append((1-b**i)/(1-b))
  b_name = str(b).replace(".","_")
  name = "Loss_Function_Evaluation_Weighted_CrossEntropyLoss_IMP_Objective_Class_Beta_" + b_name
  model_name = name
  gc.collect()
  torch.cuda.empty_cache()
  lenet = LeNet5()
  lenet.cuda()
  loss_function = Weighted_CrossEntropyLoss_IMP(weights=temp)
  print("Parameters used in this run: ")
  print(temp)
  train_model(lenet,model_name,batch_size=64, num_epochs=100, learning_rate=0.0001,loss_function=loss_function,save_output=True,print_stack_trace=False)

# FocalLoss
gamma_list = [0, 0.5, 1, 2, 5]
for g in gamma_list:
  g_name = str(g).replace(".","_")
  name = "Loss_Function_Evaluation_FocalLoss_IMP_GAMMA_" + g_name
  model_name = name
  gc.collect()
  torch.cuda.empty_cache()
  lenet = LeNet5()
  lenet.cuda()
  loss_function = FocalLoss_IMP(gamma = g)
  train_model(lenet,model_name,batch_size=64, num_epochs=100, learning_rate=0.0001,loss_function=loss_function,save_output=True,print_stack_trace=False)

# SparseMax Loss
model_name = "Loss_Function_Evaluation_SparseMax_IMP"
gc.collect()
torch.cuda.empty_cache()
lenet = LeNet5()
lenet.cuda()
loss_function = Sparsemax_Loss_IMP()
train_model(lenet,model_name,batch_size=64, num_epochs=100, learning_rate=0.0001,loss_function=loss_function,save_output=True,print_stack_trace=False)

# LDAM - Normalized
count = [98.0,2585.0,574.0]
C_list = [0.1,0.5,1]

for c in C_list:
  c_name = str(c).replace(".","_")
  name = "Loss_Function_Evaluation_LDAM_IMP_Normalized_C_" + c_name
  model_name = name
  gc.collect()
  torch.cuda.empty_cache()
  lenet = LeNet5()
  lenet.cuda()
  loss_function = LDAMLoss_IMP(n=count,C=c,normalize=True)
  train_model(lenet,model_name,batch_size=64, num_epochs=100, learning_rate=0.0001,loss_function=loss_function,save_output=True,print_stack_trace=False)

# LDAM - non_normalized
count = [98.0,2585.0,574.0]
C_list = [0.1,0.5,1]

for c in C_list:
  c_name = str(c).replace(".","_")
  name = "Loss_Function_Evaluation_LDAM_IMP_Non_Normalized_C_" + c_name
  model_name = name
  gc.collect()
  torch.cuda.empty_cache()
  lenet = LeNet5()
  lenet.cuda()
  loss_function = LDAMLoss_IMP(n=count,C=c,normalize=False)
  train_model(lenet,model_name,batch_size=64, num_epochs=100, learning_rate=0.0001,loss_function=loss_function,save_output=True,print_stack_trace=False)

""" ## Loss Function Evaluation (Final Performance - Error Rate, Acc, Confusion Matrix,Precision, Recall and F1-Score,Training Time)

#### Covergence Time
https://machinelearningmastery.com/how-to-choose-loss-functions-when-training-deep-learning-neural-networks/
"""

def Show_Specific_Training_Curve(file_name_path,loss_name):
  train_err = np.loadtxt("/content/drive/My Drive/MIE424 Final Project/model_output/{}_train_err.csv".format(file_name_path))
  val_err = np.loadtxt("/content/drive/My Drive/MIE424 Final Project/model_output/{}_val_err.csv".format(file_name_path))
  train_loss = np.loadtxt("/content/drive/My Drive/MIE424 Final Project/model_output/{}_train_loss.csv".format(file_name_path))
  val_loss = np.loadtxt("/content/drive/My Drive/MIE424 Final Project/model_output/{}_val_loss.csv".format(file_name_path))
  error_graph_title = "Train vs Validation Error ({})".format(loss_name)
  plt.title(error_graph_title)
  n = len(train_err) # number of epochs
  plt.plot(range(1,n+1), train_err, label="Train")
  plt.plot(range(1,n+1), val_err, label="Validation")
  plt.xlabel("Epoch")
  plt.ylabel("Error")
  plt.legend(loc='best')
  plt.show()
  loss_graph_title = "Train vs Validation Loss ({})".format(loss_name)
  plt.title(loss_graph_title)
  plt.plot(range(1,n+1), train_loss, label="Train")
  plt.plot(range(1,n+1), val_loss, label="Validation")
  plt.xlabel("Epoch")
  plt.ylabel("Loss")
  plt.legend(loc='best')
  plt.show()

# Plot Confusion Matrix
# Ref https://stackoverflow.com/questions/35572000/how-can-i-plot-a-confusion-matrix
def Plot_Confusion_Matrix(final_model, test_loader,loss_name,using_cifar = False):
  pred_list = []
  actual_list = []

  for i, data in enumerate(test_loader, 0):
    inputs, labels = data
    # labels = normalize_label(labels)
    inputs = inputs.cuda()
    labels = labels.cuda()
    outputs = final_model(inputs)

    _, predicted = torch.max(outputs.data, 1)
    pred_list = pred_list + predicted.tolist()
    actual_list = actual_list + labels.tolist()

  # print(pred_list)
  # print(len(pred_list))
  # print(actual_list)
  # print(len(actual_list))

  if using_cifar:
    y_pred = pred_list
    y_act = actual_list
    print(metrics.confusion_matrix(y_act, y_pred))
    print(metrics.classification_report(y_act, y_pred))
    array = metrics.confusion_matrix(y_act, y_pred, labels=[9,8,7,6,5,4,3,2,1,0])

    df_cm = pd.DataFrame(array, range(10), range(10))
    plt.figure(figsize=(10,7))
    confusion_matrix_title = "Confusion Matrix ({})".format(loss_name)
    plt.title(confusion_matrix_title)
    sn.set(font_scale=1.4) # for label size
    df_cm = pd.DataFrame(array, index=[9,8,7,6,5,4,3,2,1,0], columns=[9,8,7,6,5,4,3,2,1,0])
    sn.heatmap(df_cm, annot=True, annot_kws={"size": 12}, cmap="YlGnBu",fmt='d')

    plt.show()
  else:
    y_pred = pred_list
    y_act = actual_list
    print(metrics.confusion_matrix(y_act, y_pred))
    print(metrics.classification_report(y_act, y_pred))
    array = metrics.confusion_matrix(y_act, y_pred, labels=[2, 1, 0])
    # 0:'mask incorrectly',1:'mask',2:'no mask'

    df_cm = pd.DataFrame(array, range(3), range(3))
    plt.figure(figsize=(10,7))
    confusion_matrix_title = "Confusion Matrix ({})".format(loss_name)
    plt.title(confusion_matrix_title)
    sn.set(font_scale=1.4) # for label size
    df_cm = pd.DataFrame(array, index=["no mask","mask", "mask incorrectly"], columns=["no mask","mask", "mask incorrectly"])
    sn.heatmap(df_cm, annot=True, annot_kws={"size": 12}, cmap="YlGnBu",fmt='d')

    plt.show()

def Print_Loss_Function_Evaluation(loss_name, loss_function,test_loader, graph_path, model_path, using_cifar = False):
  if using_cifar:
    final_model = LeNet5(num_class=10)
  else:
    final_model = LeNet5()
  m_p = "/content/drive/My Drive/MIE424 Final Project/model_output/"+model_path
  final_model.load_state_dict(torch.load(m_p))
  final_model.cuda()

  final_model_test_err, final_model_test_loss = evaluate(final_model, test_loader, loss_function.cuda())
  print("The testing error of the model trained using {} is: {}".format(loss_name,final_model_test_err))
  print("The testing loss of the model trained using {} is: {}".format(loss_name,final_model_test_loss))

  Show_Specific_Training_Curve(file_name_path=graph_path,loss_name=loss_name)
  Plot_Confusion_Matrix(final_model=final_model, test_loader=test_loader, loss_name = loss_name,using_cifar=using_cifar)

train_loader, val_loader, test_loader = get_data_loader(batch_size=64)

# CE Evaluation
loss_name = "CE"
loss_function = CrossEntropyLoss_IMP()
graph_path = "model_Loss_Function_Evaluation_CrossEntropyLoss_IMP_bs64_lr0.0001_epoch99"
model_path = "Loss_Function_Evaluation_CrossEntropyLoss_IMPbest_val_err"
Print_Loss_Function_Evaluation(loss_name,loss_function,test_loader,graph_path,model_path)

# Weighted CE (Weight = Frequency) Evaluation
loss_name = "W_CE (Weight = Frequency)"
loss_function = Weighted_CrossEntropyLoss_IMP(weights=[98.0/3257.0,2585.0/3257.0,574.0/3257.0])
graph_path = "model_Loss_Function_Evaluation_Weighted_CrossEntropyLoss_IMP_Frequency_bs64_lr0.0001_epoch99"
model_path = "Loss_Function_Evaluation_Weighted_CrossEntropyLoss_IMP_Frequencybest_val_loss"
Print_Loss_Function_Evaluation(loss_name,loss_function,test_loader,graph_path,model_path)

# Weighted CE (Weight = Inverse class frequency) Evaluation
loss_name = "W_CE (Weight = Inverse class frequency)"
loss_function = Weighted_CrossEntropyLoss_IMP(weights=[3257.0/98.0,3257.0/2585.0,3257.0/574.0])
graph_path = "model_Loss_Function_Evaluation_Weighted_CrossEntropyLoss_IMP_Inverse_Frequency_bs64_lr0.0001_epoch99"
model_path = "Loss_Function_Evaluation_Weighted_CrossEntropyLoss_IMP_Inverse_Frequencybest_val_loss"
Print_Loss_Function_Evaluation(loss_name,loss_function,test_loader,graph_path,model_path)

# Weighted CE (Weight = Linearly scaled k>0) Evaluation
K = [0.1, 0.5, 1.2, 5, 10]

count = [98.0,2585.0,574.0]
weights_frequency = []
for i in count:
  weights_frequency.append(i/sum(count))

for k in K:
  temp = []
  for i in weights_frequency:
    temp.append(k/i)
  k_name = str(k).replace(".","_")
  loss_name = "W_CE (Weight = Linearly scaled; k = {})".format(k_name)
  loss_function = Weighted_CrossEntropyLoss_IMP(weights=temp)
  graph_path = "model_Loss_Function_Evaluation_Weighted_CrossEntropyLoss_IMP_Linearly_Scaled_K_{}_bs64_lr0.0001_epoch99".format(k_name)
  model_path = "Loss_Function_Evaluation_Weighted_CrossEntropyLoss_IMP_Linearly_Scaled_K_{}best_val_loss".format(k_name)
  Print_Loss_Function_Evaluation(loss_name,loss_function,test_loader,graph_path,model_path)

# Weighted CE (Weight = logarithmic q>frequency) Evaluation
# Remove 0.5 here because the result is infeasible
Q = [1, 2, 5, 10]

count = [98.0,2585.0,574.0]
weights_frequency = []
for i in count:
  weights_frequency.append(i/sum(count))

for q in Q:
  temp = []
  for i in weights_frequency:
    temp.append(math.log(q/i))
  q_name = str(q).replace(".","_")
  loss_name = "W_CE (Weight = logarithmic; q = {})".format(q_name)
  loss_function = Weighted_CrossEntropyLoss_IMP(weights=temp)
  graph_path = "model_Loss_Function_Evaluation_Weighted_CrossEntropyLoss_IMP_Logarithmic_Q_{}_bs64_lr0.0001_epoch99".format(q_name)
  model_path = "Loss_Function_Evaluation_Weighted_CrossEntropyLoss_IMP_Logarithmic_Q_{}best_val_loss".format(q_name)
  Print_Loss_Function_Evaluation(loss_name,loss_function,test_loader,graph_path,model_path)

# Weighted CE (Weight = Effective number of samples) Evaluation
Betas = [0.9,0.9,0.999,0.9999]
count = [98.0,2585.0,574.0]
for b in Betas:
  temp = []
  for i in count:
    temp.append((1-b**i)/(1-b))
  b_name = str(b).replace(".","_")
  loss_name = "W_CE (Weight = Effective number of samples; beta = {})".format(b_name)
  loss_function = Weighted_CrossEntropyLoss_IMP(weights=temp)
  graph_path = "model_Loss_Function_Evaluation_Weighted_CrossEntropyLoss_IMP_Objective_Class_Beta_{}_bs64_lr0.0001_epoch99".format(b_name)
  model_path = "Loss_Function_Evaluation_Weighted_CrossEntropyLoss_IMP_Objective_Class_Beta_{}best_val_loss".format(b_name)
  Print_Loss_Function_Evaluation(loss_name,loss_function,test_loader,graph_path,model_path)

# FocalLoss Evaluation
gamma_list = [0, 0.5, 1, 2, 5]
for g in gamma_list:
  g_name = str(g).replace(".","_")
  loss_name = "FocalLoss (gamma = {})".format(g_name)
  loss_function = FocalLoss_IMP(gamma = g)
  graph_path = "model_Loss_Function_Evaluation_FocalLoss_IMP_GAMMA_{}_bs64_lr0.0001_epoch99".format(g_name)
  model_path = "Loss_Function_Evaluation_FocalLoss_IMP_GAMMA_{}best_val_loss".format(g_name)
  Print_Loss_Function_Evaluation(loss_name,loss_function,test_loader,graph_path,model_path)

# SparseMax Loss Evaluation
loss_name = "SparseMax Loss"
loss_function = Sparsemax_Loss_IMP()
graph_path = "model_Loss_Function_Evaluation_SparseMax_IMP_bs64_lr0.0001_epoch99"
model_path = "Loss_Function_Evaluation_SparseMax_IMPbest_val_loss"
Print_Loss_Function_Evaluation(loss_name,loss_function,test_loader,graph_path,model_path)

# LDAM - Normalized Evaluation
count = [98.0,2585.0,574.0]
C_list = [0.1,0.5,1]

for c in C_list:
  c_name = str(c).replace(".","_")
  loss_name = "LDAM (Normalized; C = {})".format(c_name)
  loss_function = LDAMLoss_IMP(n=count,C=c,normalize=True)
  graph_path = "model_Loss_Function_Evaluation_LDAM_IMP_Normalized_C_{}_bs64_lr0.0001_epoch99".format(c_name)
  model_path = "Loss_Function_Evaluation_LDAM_IMP_Normalized_C_{}best_val_loss".format(c_name)
  Print_Loss_Function_Evaluation(loss_name,loss_function,test_loader,graph_path,model_path)

# LDAM - Non Normalized Evaluation
count = [98.0,2585.0,574.0]
C_list = [0.1,0.5,1]

for c in C_list:
  c_name = str(c).replace(".","_")
  loss_name = "LDAM (Non Normalized; C = {})".format(c_name)
  loss_function = LDAMLoss_IMP(n=count,C=c,normalize=False)
  graph_path = "model_Loss_Function_Evaluation_LDAM_IMP_Non_Normalized_C_{}_bs64_lr0.0001_epoch99".format(c_name)
  model_path = "Loss_Function_Evaluation_LDAM_IMP_Non_Normalized_C_{}best_val_loss".format(c_name)
  Print_Loss_Function_Evaluation(loss_name,loss_function,test_loader,graph_path,model_path)

"""## Test on CIFAR-10 LT"""

# This helper function to generate imbalanced CIFAR 10 dataset is from KaihuaTang
# REF: https://github.com/KaihuaTang/Long-Tailed-Recognition.pytorch/blob/master/classification/data/ImbalanceCIFAR.py

transform_function = transforms.Compose(
        [
         transforms.Resize((50,50)),
         transforms.ToTensor(),
         ])

class IMBALANCECIFAR10(torchvision.datasets.CIFAR10):
    cls_num = 10

    def __init__(self, root, imb_type='exp', imb_factor=0.01, rand_number=0, train=True,
                 transform=None, target_transform=None,
                 download=False):
        super(IMBALANCECIFAR10, self).__init__(root, train, transform, target_transform, download)
        np.random.seed(rand_number)
        img_num_list = self.get_img_num_per_cls(self.cls_num, imb_type, imb_factor)
        self.gen_imbalanced_data(img_num_list)

    def get_img_num_per_cls(self, cls_num, imb_type, imb_factor):
        img_max = len(self.data) / cls_num
        img_num_per_cls = []
        if imb_type == 'exp':
            for cls_idx in range(cls_num):
                num = img_max * (imb_factor**(cls_idx / (cls_num - 1.0)))
                img_num_per_cls.append(int(num))
        elif imb_type == 'step':
            for cls_idx in range(cls_num // 2):
                img_num_per_cls.append(int(img_max))
            for cls_idx in range(cls_num // 2):
                img_num_per_cls.append(int(img_max * imb_factor))
        else:
            img_num_per_cls.extend([int(img_max)] * cls_num)
        return img_num_per_cls

    def gen_imbalanced_data(self, img_num_per_cls):
        new_data = []
        new_targets = []
        targets_np = np.array(self.targets, dtype=np.int64)
        classes = np.unique(targets_np)
        # np.random.shuffle(classes)
        self.num_per_cls_dict = dict()
        for the_class, the_img_num in zip(classes, img_num_per_cls):
            self.num_per_cls_dict[the_class] = the_img_num
            idx = np.where(targets_np == the_class)[0]
            np.random.shuffle(idx)
            selec_idx = idx[:the_img_num]
            new_data.append(self.data[selec_idx, ...])
            new_targets.extend([the_class, ] * the_img_num)
        new_data = np.vstack(new_data)
        self.data = new_data
        self.targets = new_targets
        
    def get_cls_num_list(self):
        cls_num_list = []
        for i in range(self.cls_num):
            cls_num_list.append(self.num_per_cls_dict[i])
        return cls_num_list

# Initialize the dataset and download the training_validation image
train_validation_dataset = IMBALANCECIFAR10(root='/content/drive/My Drive/MIE424 Final Project/CIFAR_10', imb_type="exp", imb_factor=0.01, rand_number=66, train=True, download=True, transform=transform_function)

# Initialize the dataset and download the testing image
testing_dataset = IMBALANCECIFAR10(root='/content/drive/My Drive/MIE424 Final Project/CIFAR_10_Testing', imb_type="exp", imb_factor=0.01, rand_number=66, train=False, download=True, transform=transform_function)

len(testing_dataset)

x = [1,2,3,4,5,6,7,8,9,10]
y = []
for i in range(10):
  num = train_validation_dataset.get_cls_num_list()[i] + testing_dataset.get_cls_num_list()[i]
  y.append(num)
a = train_validation_dataset.get_cls_num_list() + testing_dataset.get_cls_num_list()

print(x)
print(y)
print(a)

def addlabels(x,y):
    for i in range(len(x)):
        plt.text(i,y[i],y[i])

fig, ax = plt.subplots(figsize=(8,5))
ax.bar(x,y)
ax.set_xticks(x)
addlabels(x, y)
plt.show()

def get_data_loader_cifar_10_LT(batch_size):
  full_training_validation_data = IMBALANCECIFAR10(root='/content/drive/My Drive/MIE424 Final Project/CIFAR_10', imb_type="exp", imb_factor=0.01, rand_number=66, train=True, download=True, transform=transform_function)
  full_testing_data = IMBALANCECIFAR10(root='/content/drive/My Drive/MIE424 Final Project/CIFAR_10_Testing', imb_type="exp", imb_factor=0.01, rand_number=66, train=False, download=True, transform=transform_function)
  
  # run train_test_split twice to stratified split dataset into train,validation and testing
  targets = full_training_validation_data.targets
  # print(dict(Counter(targets)))
  train_idx, valid_test_idx= train_test_split(np.arange(len(targets)),test_size=0.15,shuffle=True,stratify=targets)
  train_targets = [targets[i] for i in train_idx]
  # print(dict(Counter(train_targets)))
  train_data = Custom_Subset(full_training_validation_data,train_idx,train_targets)
  valid_targets = [targets[i] for i in valid_test_idx]
  # print(dict(Counter(valid_test_targets)))
  valid_data = Custom_Subset(full_training_validation_data,valid_test_idx,valid_targets)


  print("There are " + str(len(train_data)) + " training images, and the corresponding data distribution is: ")
  print(dict(Counter(train_data.targets)))
  print("There are " + str(len(valid_data)) + " validation images, and the corresponding data distribution is: ")
  print(dict(Counter(valid_data.targets)))

  print("There are " + str(len(full_testing_data)) + " test images, and the corresponding data distribution is: ")
  print(dict(Counter(full_testing_data.targets)))
  
  train_loader = torch.utils.data.DataLoader(train_data, batch_size=batch_size,num_workers=2)
  
  val_loader = torch.utils.data.DataLoader(valid_data, batch_size=batch_size,num_workers=2)

  test_loader = torch.utils.data.DataLoader(full_testing_data, batch_size=batch_size,num_workers=2)
  
  return train_loader, val_loader, test_loader

train_loader, val_loader, test_loader = get_data_loader_cifar_10_LT(batch_size=64)

class_dict = {2: 1527, 6: 197, 0: 4250, 1: 2547, 3: 915, 5: 329, 4: 548, 9: 43, 7: 118, 8: 71}
count = []
for i in range(10):
  count.append(class_dict[i])
weights_frequency = []
for i in count:
  weights_frequency.append(i/sum(count))
weights_inv_frequency = []
for i in weights_frequency:
  weights_inv_frequency.append(1.0/i)

print(count)
print(weights_frequency)
print(weights_inv_frequency)

# CE Long_Tailed_CIFAR
model_name = "CIFAR_LT_Loss_Function_Evaluation_CrossEntropyLoss_IMP"
gc.collect()
torch.cuda.empty_cache()
lenet = LeNet5(num_class=10)
lenet.cuda()
loss_function = CrossEntropyLoss_IMP()
train_model(lenet,model_name,batch_size=64, num_epochs=100, learning_rate=0.0001,loss_function=loss_function,save_output=True,print_stack_trace=False,using_cifar = True)

# Weighted CE (Weight = Frequency) Long_Tailed_CIFAR
model_name = "CIFAR_LT_Loss_Function_Evaluation_Weighted_CrossEntropyLoss_IMP_Frequency"
gc.collect()
torch.cuda.empty_cache()
lenet = LeNet5(num_class=10)
lenet.cuda()
loss_function = Weighted_CrossEntropyLoss_IMP(weights=weights_frequency)
train_model(lenet,model_name,batch_size=64, num_epochs=100, learning_rate=0.0001,loss_function=loss_function,save_output=True,print_stack_trace=False,using_cifar = True)
# Weighted CE (Weight = Inverse class frequency)
model_name = "CIFAR_LT_Loss_Function_Evaluation_Weighted_CrossEntropyLoss_IMP_Inverse_Frequency"
gc.collect()
torch.cuda.empty_cache()
lenet = LeNet5(num_class=10)
lenet.cuda()
loss_function = Weighted_CrossEntropyLoss_IMP(weights=weights_inv_frequency)
train_model(lenet,model_name,batch_size=64, num_epochs=100, learning_rate=0.0001,loss_function=loss_function,save_output=True,print_stack_trace=False,using_cifar = True)

# Weighted CE (Weight = Linearly scaled k>0) Long_Tailed_CIFAR
K = [0.1, 0.5, 1.2, 5, 10]
for k in K:
  temp = []
  for i in weights_frequency:
    temp.append(k/i)
  k_name = str(k).replace(".","_")
  name = "CIFAR_LT_Loss_Function_Evaluation_Weighted_CrossEntropyLoss_IMP_Linearly_Scaled_K_" + k_name
  model_name = name
  gc.collect()
  torch.cuda.empty_cache()
  lenet = LeNet5(num_class=10)
  lenet.cuda()
  loss_function = Weighted_CrossEntropyLoss_IMP(weights=temp)
  print("Parameters used in this run: ")
  print(temp)
  train_model(lenet,model_name,batch_size=64, num_epochs=100, learning_rate=0.0001,loss_function=loss_function,save_output=True,print_stack_trace=False,using_cifar = True)

# Weighted CE (Weight = logarithmic q>frequency) Long_Tailed_CIFAR
Q = [0.5, 1, 2, 5, 10]
for q in Q:
  temp = []
  for i in weights_frequency:
    temp.append(math.log(q/i))
  q_name = str(q).replace(".","_")
  name = "CIFAR_LT_Loss_Function_Evaluation_Weighted_CrossEntropyLoss_IMP_Logarithmic_Q_" + q_name
  model_name = name
  gc.collect()
  torch.cuda.empty_cache()
  lenet = LeNet5(num_class=10)
  lenet.cuda()
  loss_function = Weighted_CrossEntropyLoss_IMP(weights=temp)
  print("Parameters used in this run: ")
  print(temp)
  train_model(lenet,model_name,batch_size=64, num_epochs=100, learning_rate=0.0001,loss_function=loss_function,save_output=True,print_stack_trace=False,using_cifar = True)

# Weighted CE (Weight = Effective number of samples) Long_Tailed_CIFAR
Betas = [0.9,0.9,0.999,0.9999]
for b in Betas:
  temp = []
  for i in count:
    temp.append((1-b**i)/(1-b))
  b_name = str(b).replace(".","_")
  name = "CIFAR_LT_Loss_Function_Evaluation_Weighted_CrossEntropyLoss_IMP_Objective_Class_Beta_" + b_name
  model_name = name
  gc.collect()
  torch.cuda.empty_cache()
  lenet = LeNet5(num_class=10)
  lenet.cuda()
  loss_function = Weighted_CrossEntropyLoss_IMP(weights=temp)
  print("Parameters used in this run: ")
  print(temp)
  train_model(lenet,model_name,batch_size=64, num_epochs=100, learning_rate=0.0001,loss_function=loss_function,save_output=True,print_stack_trace=False,using_cifar = True)

# FocalLoss Long_Tailed_CIFAR
gamma_list = [0, 0.5, 1, 2, 5]
for g in gamma_list:
  g_name = str(g).replace(".","_")
  name = "CIFAR_LT_Loss_Function_Evaluation_FocalLoss_IMP_GAMMA_" + g_name
  model_name = name
  gc.collect()
  torch.cuda.empty_cache()
  lenet = LeNet5(num_class=10)
  lenet.cuda()
  loss_function = FocalLoss_IMP(gamma = g)
  train_model(lenet,model_name,batch_size=64, num_epochs=100, learning_rate=0.0001,loss_function=loss_function,save_output=True,print_stack_trace=False,using_cifar = True)

# SparseMax Loss Long_Tailed_CIFAR
model_name = "CIFAR_LT_Loss_Function_Evaluation_SparseMax_IMP"
gc.collect()
torch.cuda.empty_cache()
lenet = LeNet5(num_class=10)
lenet.cuda()
loss_function = Sparsemax_Loss_IMP()
train_model(lenet,model_name,batch_size=64, num_epochs=100, learning_rate=0.0001,loss_function=loss_function,save_output=True,print_stack_trace=False,using_cifar = True)

# LDAM - Normalized Long_Tailed_CIFAR
C_list = [0.1,0.5,1]

for c in C_list:
  c_name = str(c).replace(".","_")
  name = "CIFAR_LT_Loss_Function_Evaluation_LDAM_IMP_Normalized_C_" + c_name
  model_name = name
  gc.collect()
  torch.cuda.empty_cache()
  lenet = LeNet5(num_class=10)
  lenet.cuda()
  loss_function = LDAMLoss_IMP(n=count,C=c,normalize=True)
  train_model(lenet,model_name,batch_size=64, num_epochs=100, learning_rate=0.0001,loss_function=loss_function,save_output=True,print_stack_trace=False,using_cifar = True)

# LDAM - Non Normalized Long_Tailed_CIFAR
C_list = [0.1,0.5,1]

for c in C_list:
  c_name = str(c).replace(".","_")
  name = "CIFAR_LT_Loss_Function_Evaluation_LDAM_IMP_Non_Normalized_C_" + c_name
  model_name = name
  gc.collect()
  torch.cuda.empty_cache()
  lenet = LeNet5(num_class=10)
  lenet.cuda()
  loss_function = LDAMLoss_IMP(n=count,C=c,normalize=False)
  train_model(lenet,model_name,batch_size=64, num_epochs=100, learning_rate=0.0001,loss_function=loss_function,save_output=True,print_stack_trace=False,using_cifar = True)

"""## CIFAR 10 Evaluation Results"""

# CE Evaluation
loss_name = "CE CIFAR_LT"
loss_function = CrossEntropyLoss_IMP()
graph_path = "model_CIFAR_LT_Loss_Function_Evaluation_CrossEntropyLoss_IMP_bs64_lr0.0001_epoch99"
model_path = "CIFAR_LT_Loss_Function_Evaluation_CrossEntropyLoss_IMPbest_val_err"
Print_Loss_Function_Evaluation(loss_name,loss_function,test_loader,graph_path,model_path,using_cifar = True)

# Weighted CE (Weight = Frequency) Evaluation
loss_name = "W_CE CIFAR_LT (Weight = Frequency)"
loss_function = Weighted_CrossEntropyLoss_IMP(weights=weights_frequency)
graph_path = "model_CIFAR_LT_Loss_Function_Evaluation_Weighted_CrossEntropyLoss_IMP_Frequency_bs64_lr0.0001_epoch99"
model_path = "CIFAR_LT_Loss_Function_Evaluation_Weighted_CrossEntropyLoss_IMP_Frequencybest_val_loss"
Print_Loss_Function_Evaluation(loss_name,loss_function,test_loader,graph_path,model_path,using_cifar = True)

# Weighted CE (Weight = Inverse class frequency) Evaluation
loss_name = "W_CE CIFAR_LT (Weight = Inverse class frequency)"
loss_function = Weighted_CrossEntropyLoss_IMP(weights=weights_inv_frequency)
graph_path = "model_CIFAR_LT_Loss_Function_Evaluation_Weighted_CrossEntropyLoss_IMP_Inverse_Frequency_bs64_lr0.0001_epoch99"
model_path = "CIFAR_LT_Loss_Function_Evaluation_Weighted_CrossEntropyLoss_IMP_Inverse_Frequencybest_val_loss"
Print_Loss_Function_Evaluation(loss_name,loss_function,test_loader,graph_path,model_path,using_cifar = True)

# Weighted CE (Weight = Linearly scaled k>0) Evaluation
K = [0.1, 0.5, 1.2, 5, 10]
weights_frequency = []
for i in count:
  weights_frequency.append(i/sum(count))

for k in K:
  temp = []
  for i in weights_frequency:
    temp.append(k/i)
  k_name = str(k).replace(".","_")
  loss_name = "W_CE CIFAR_LT (Weight = Linearly scaled; k = {})".format(k_name)
  loss_function = Weighted_CrossEntropyLoss_IMP(weights=temp)
  graph_path = "model_CIFAR_LT_Loss_Function_Evaluation_Weighted_CrossEntropyLoss_IMP_Linearly_Scaled_K_{}_bs64_lr0.0001_epoch99".format(k_name)
  model_path = "CIFAR_LT_Loss_Function_Evaluation_Weighted_CrossEntropyLoss_IMP_Linearly_Scaled_K_{}best_val_loss".format(k_name)
  Print_Loss_Function_Evaluation(loss_name,loss_function,test_loader,graph_path,model_path,using_cifar=True)

# Weighted CE (Weight = logarithmic q>frequency) Evaluation
# Remove 0.5 here because the result is infeasible
Q = [1, 2, 5, 10]
weights_frequency = []
for i in count:
  weights_frequency.append(i/sum(count))

for q in Q:
  temp = []
  for i in weights_frequency:
    temp.append(math.log(q/i))
  q_name = str(q).replace(".","_")
  loss_name = "W_CE CIFAR_LT (Weight = logarithmic; q = {})".format(q_name)
  loss_function = Weighted_CrossEntropyLoss_IMP(weights=temp)
  graph_path = "model_CIFAR_LT_Loss_Function_Evaluation_Weighted_CrossEntropyLoss_IMP_Logarithmic_Q_{}_bs64_lr0.0001_epoch99".format(q_name)
  model_path = "CIFAR_LT_Loss_Function_Evaluation_Weighted_CrossEntropyLoss_IMP_Logarithmic_Q_{}best_val_loss".format(q_name)
  Print_Loss_Function_Evaluation(loss_name,loss_function,test_loader,graph_path,model_path,using_cifar=True)

# Weighted CE (Weight = Effective number of samples) Evaluation
Betas = [0.9,0.9,0.999,0.9999]
for b in Betas:
  temp = []
  for i in count:
    temp.append((1-b**i)/(1-b))
  b_name = str(b).replace(".","_")
  loss_name = "W_CE CIFAR_LT (Weight = Effective number of samples; beta = {})".format(b_name)
  loss_function = Weighted_CrossEntropyLoss_IMP(weights=temp)
  graph_path = "model_CIFAR_LT_Loss_Function_Evaluation_Weighted_CrossEntropyLoss_IMP_Objective_Class_Beta_{}_bs64_lr0.0001_epoch99".format(b_name)
  model_path = "CIFAR_LT_Loss_Function_Evaluation_Weighted_CrossEntropyLoss_IMP_Objective_Class_Beta_{}best_val_loss".format(b_name)
  Print_Loss_Function_Evaluation(loss_name,loss_function,test_loader,graph_path,model_path,using_cifar=True)

# FocalLoss Evaluation
gamma_list = [0, 0.5, 1, 2, 5]
for g in gamma_list:
  g_name = str(g).replace(".","_")
  loss_name = "FocalLoss CIFAR_LT (gamma = {})".format(g_name)
  loss_function = FocalLoss_IMP(gamma = g)
  graph_path = "model_CIFAR_LT_Loss_Function_Evaluation_FocalLoss_IMP_GAMMA_{}_bs64_lr0.0001_epoch99".format(g_name)
  model_path = "CIFAR_LT_Loss_Function_Evaluation_FocalLoss_IMP_GAMMA_{}best_val_loss".format(g_name)
  Print_Loss_Function_Evaluation(loss_name,loss_function,test_loader,graph_path,model_path,using_cifar=True)

# SparseMax Loss Evaluation
loss_name = "SparseMax CIFAR_LT Loss"
loss_function = Sparsemax_Loss_IMP()
graph_path = "model_CIFAR_LT_Loss_Function_Evaluation_SparseMax_IMP_bs64_lr0.0001_epoch99"
model_path = "CIFAR_LT_Loss_Function_Evaluation_SparseMax_IMPbest_val_loss"
Print_Loss_Function_Evaluation(loss_name,loss_function,test_loader,graph_path,model_path,using_cifar=True)

# LDAM - Normalized Evaluation
C_list = [0.1,0.5,1]

for c in C_list:
  c_name = str(c).replace(".","_")
  loss_name = "LDAM CIFAR_LT (Normalized; C = {})".format(c_name)
  loss_function = LDAMLoss_IMP(n=count,C=c,normalize=True)
  graph_path = "model_CIFAR_LT_Loss_Function_Evaluation_LDAM_IMP_Normalized_C_{}_bs64_lr0.0001_epoch99".format(c_name)
  model_path = "CIFAR_LT_Loss_Function_Evaluation_LDAM_IMP_Normalized_C_{}best_val_loss".format(c_name)
  Print_Loss_Function_Evaluation(loss_name,loss_function,test_loader,graph_path,model_path,using_cifar=True)

# LDAM - Non Normalized Evaluation
C_list = [0.1,0.5,1]

for c in C_list:
  c_name = str(c).replace(".","_")
  loss_name = "LDAM CIFAR_LT (Non Normalized; C = {})".format(c_name)
  loss_function = LDAMLoss_IMP(n=count,C=c,normalize=False)
  graph_path = "model_CIFAR_LT_Loss_Function_Evaluation_LDAM_IMP_Non_Normalized_C_{}_bs64_lr0.0001_epoch99".format(c_name)
  model_path = "CIFAR_LT_Loss_Function_Evaluation_LDAM_IMP_Non_Normalized_C_{}best_val_loss".format(c_name)
  Print_Loss_Function_Evaluation(loss_name,loss_function,test_loader,graph_path,model_path,using_cifar=True)